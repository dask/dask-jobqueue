jobqueue:
  oar:
    name: dask-worker

    # Dask worker options
    cores: null                 # Total number of cores per job
    memory: null                # Total amount of memory per job
    processes: null                # Number of Python processes per job

    interface: null             # Network interface to use like eth0 or ib0
    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
    local-directory: null       # Location of fast local storage like /scratch or $TMPDIR
    extra: []

    # OAR resource manager options
    shebang: "#!/usr/bin/env bash"
    queue: null
    project: null
    walltime: '00:30:00'
    env-extra: []
    resource-spec: null
    job-extra: []
    log-directory: null
    
    # Scheduler options
    scheduler-options: {}

  pbs:
    name: dask-worker

    # Dask worker options
    cores: null                 # Total number of cores per job
    memory: null                # Total amount of memory per job
    processes: null                # Number of Python processes per job

    interface: null             # Network interface to use like eth0 or ib0
    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
    local-directory: null       # Location of fast local storage like /scratch or $TMPDIR
    extra: []

    # PBS resource manager options
    shebang: "#!/usr/bin/env bash"
    queue: null
    project: null
    walltime: '00:30:00'
    env-extra: []
    resource-spec: null
    job-extra: []
    log-directory: null
    
    # Scheduler options
    scheduler-options: {}

  sge:
    name: dask-worker

    # Dask worker options
    cores: null                 # Total number of cores per job
    memory: null                # Total amount of memory per job
    processes: null                # Number of Python processes per job

    interface: null             # Network interface to use like eth0 or ib0
    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
    local-directory: null       # Location of fast local storage like /scratch or $TMPDIR
    extra: []

    # SGE resource manager options
    shebang: "#!/usr/bin/env bash"
    queue: null
    project: null
    walltime: '00:30:00'
    env-extra: []
    job-extra: []
    log-directory: null
    resource-spec: null
    
    # Scheduler options
    scheduler-options: {}

  slurm:
    name: dask-worker

    # Dask worker options
    cores: null                 # Total number of cores per job
    memory: null                # Total amount of memory per job
    processes: null                # Number of Python processes per job

    interface: null             # Network interface to use like eth0 or ib0
    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
    local-directory: null       # Location of fast local storage like /scratch or $TMPDIR
    extra: []

    # SLURM resource manager options
    shebang: "#!/usr/bin/env bash"
    queue: null
    project: null
    walltime: '00:30:00'
    env-extra: []
    job-cpu: null
    job-mem: null
    job-extra: []
    log-directory: null
    
    # Scheduler options
    scheduler-options: {}

  moab:
    name: dask-worker

    # Dask worker options
    cores: null                 # Total number of cores per job
    memory: null                # Total amount of memory per job
    processes: null                # Number of Python processes per job

    interface: null             # Network interface to use like eth0 or ib0
    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
    local-directory: null       # Location of fast local storage like /scratch or $TMPDIR
    extra: []

    # PBS resource manager options
    shebang: "#!/usr/bin/env bash"
    queue: null
    project: null
    walltime: '00:30:00'
    env-extra: []
    resource-spec: null
    job-extra: []
    log-directory: null
    
    # Scheduler options
    scheduler-options: {}

  lsf:
    name: dask-worker

    # Dask worker options
    cores: null                 # Total number of cores per job
    memory: null                # Total amount of memory per job
    processes: null                # Number of Python processes per job

    interface: null             # Network interface to use like eth0 or ib0
    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
    local-directory: null       # Location of fast local storage like /scratch or $TMPDIR
    extra: []

    # LSF resource manager options
    shebang: "#!/usr/bin/env bash"
    queue: null
    project: null
    walltime: '00:30'
    env-extra: []
    ncpus: null
    mem: null
    job-extra: []
    log-directory: null
    lsf-units: null
    use-stdin: True             # (bool) How jobs are launched, i.e. 'bsub jobscript.sh' or 'bsub < jobscript.sh'
    
    # Scheduler options
    scheduler-options: {}

  htcondor:
    name: dask-worker

    # Dask worker options
    cores: null                 # Total number of cores per job
    memory: null                # Total amount of memory per job
    processes: null                # Number of Python processes per job

    interface: null             # Network interface to use like eth0 or ib0
    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
    local-directory: null       # Location of fast local storage like /scratch or $TMPDIR
    extra: []

    # HTCondor Resource Manager options
    disk: null                  # Total amount of disk per job
    env-extra: []
    job-extra: {}               # Extra submit attributes
    log-directory: null
    shebang: "#!/usr/bin/env condor_submit"
    
    # Scheduler options
    scheduler-options: {}

  local:
    name: dask-worker
    # Dask worker options
    cores: null                 # Total number of cores per job
    memory: null                # Total amount of memory per job
    processes: null                # Number of Python processes per job

    interface: null             # Network interface to use like eth0 or ib0
    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
    local-directory: null       # Location of fast local storage like /scratch or $TMPDIR
    extra: []

    env-extra: []
    job-extra: []
    log-directory: null
    
    # Scheduler options
    scheduler-options: {}
    
  conductor:
    name: dask-worker
    #Dask worker options
    type: CPU                # Worker type CPU or GPU
    gpu-mode: null           # GPU mode DEFAULT or EXCLUSIVE
    worker-command: null     # Worker command if not dask-worker or dask-cuda-worker
    start-workers: 0         # Number of workers to start initially
    adapt: true              # Adaptive workers, true or false
    min-workers: 0           # Minimum number of workers to keep started
    max-workers: null        # Maximum number of workers to start
    slots: 1                 # Number of slots per worker
    threads: 1
    memlimit: 1GB
    processes: 1
    conda-env-path: null     # Conda env path to use for Dask workers
  
    interface: null
    resources: null
    death-timeout: 20
    lifetime: null
    local-directory: null
    
    extra: []
    env-extra: []
    log-directory: null
    silence-logs: error
   
    #Conductor options
    instance-group: null    #Instance group name on IBM Spectrum Conductor
    failed-job-limit: 5     #Max number of times to attempt to start job before failing
    job-max-retries: 5
