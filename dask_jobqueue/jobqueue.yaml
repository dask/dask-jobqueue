jobqueue:
  oar:
    name: dask-worker

    # Dask worker options
    cores: null                 # Total number of cores per job
    memory: null                # Total amount of memory per job
    processes: null                # Number of Python processes per job

    python: null                # Python executable
    interface: null             # Network interface to use like eth0 or ib0
    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
    local-directory: null       # Location of fast local storage like /scratch or $TMPDIR
    shared-temp-directory: null       # Shared directory currently used to dump temporary security objects for workers
    extra: null                 # deprecated: use worker-extra-args
    worker-command: "distributed.cli.dask_worker" # Command to launch a worker
    worker-extra-args: []       # Additional arguments to pass to `dask-worker`

    # OAR resource manager options
    shebang: "#!/usr/bin/env bash"
    queue: null
    project: null
    walltime: '00:30:00'
    env-extra: null
    job-script-prologue: []
    resource-spec: null
    job-extra: null
    job-extra-directives: []
    job-directives-skip: []
    log-directory: null
    memory-per-core-property-name: null

    # Scheduler options
    scheduler-options: {}

  pbs:
    name: dask-worker

    # Dask worker options
    cores: null                 # Total number of cores per job
    memory: null                # Total amount of memory per job
    processes: null                # Number of Python processes per job

    python: null                # Python executable
    interface: null             # Network interface to use like eth0 or ib0
    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
    local-directory: null       # Location of fast local storage like /scratch or $TMPDIR
    shared-temp-directory: null       # Shared directory currently used to dump temporary security objects for workers
    extra: null                 # deprecated: use worker-extra-args
    worker-command: "distributed.cli.dask_worker" # Command to launch a worker
    worker-extra-args: []       # Additional arguments to pass to `dask-worker`

    # PBS resource manager options
    shebang: "#!/usr/bin/env bash"
    queue: null
    account: null
    walltime: '00:30:00'
    env-extra: null
    job-script-prologue: []
    resource-spec: null
    job-extra: null
    job-extra-directives: []
    job-directives-skip: []
    log-directory: null

    # Scheduler options
    scheduler-options: {}

  sge:
    name: dask-worker

    # Dask worker options
    cores: null                 # Total number of cores per job
    memory: null                # Total amount of memory per job
    processes: null                # Number of Python processes per job

    python: null                # Python executable
    interface: null             # Network interface to use like eth0 or ib0
    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
    local-directory: null       # Location of fast local storage like /scratch or $TMPDIR
    shared-temp-directory: null       # Shared directory currently used to dump temporary security objects for workers
    extra: null                 # deprecated: use worker-extra-args
    worker-command: "distributed.cli.dask_worker" # Command to launch a worker
    worker-extra-args: []       # Additional arguments to pass to `dask-worker`

    # SGE resource manager options
    shebang: "#!/usr/bin/env bash"
    queue: null
    project: null
    walltime: '00:30:00'
    env-extra: null
    job-script-prologue: []
    job-extra: null
    job-extra-directives: []
    job-directives-skip: []
    log-directory: null
    resource-spec: null

    # Scheduler options
    scheduler-options: {}

  slurm:
    name: dask-worker

    # Dask worker options
    cores: null                 # Total number of cores per job
    memory: null                # Total amount of memory per job
    processes: null                # Number of Python processes per job

    python: null                # Python executable
    interface: null             # Network interface to use like eth0 or ib0
    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
    local-directory: null       # Location of fast local storage like /scratch or $TMPDIR
    shared-temp-directory: null       # Shared directory currently used to dump temporary security objects for workers
    extra: null                 # deprecated: use worker-extra-args
    worker-command: "distributed.cli.dask_worker" # Command to launch a worker
    worker-extra-args: []       # Additional arguments to pass to `dask-worker`

    # SLURM resource manager options
    shebang: "#!/usr/bin/env bash"
    queue: null
    account: null
    walltime: '00:30:00'
    env-extra: null
    job-script-prologue: []
    job-cpu: null
    job-mem: null
    job-extra: null
    job-extra-directives: []
    job-directives-skip: []
    log-directory: null

    # Scheduler options
    scheduler-options: {}

  moab:
    name: dask-worker

    # Dask worker options
    cores: null                 # Total number of cores per job
    memory: null                # Total amount of memory per job
    processes: null                # Number of Python processes per job

    python: null                # Python executable
    interface: null             # Network interface to use like eth0 or ib0
    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
    local-directory: null       # Location of fast local storage like /scratch or $TMPDIR
    shared-temp-directory: null       # Shared directory currently used to dump temporary security objects for workers
    extra: null                 # deprecated: use worker-extra-args
    worker-command: "distributed.cli.dask_worker" # Command to launch a worker
    worker-extra-args: []       # Additional arguments to pass to `dask-worker`

    # PBS resource manager options
    shebang: "#!/usr/bin/env bash"
    queue: null
    account: null
    walltime: '00:30:00'
    env-extra: null
    job-script-prologue: []
    resource-spec: null
    job-extra: null
    job-extra-directives: []
    job-directives-skip: []
    log-directory: null

    # Scheduler options
    scheduler-options: {}

  lsf:
    name: dask-worker

    # Dask worker options
    cores: null                 # Total number of cores per job
    memory: null                # Total amount of memory per job
    processes: null                # Number of Python processes per job

    python: null                # Python executable
    interface: null             # Network interface to use like eth0 or ib0
    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
    local-directory: null       # Location of fast local storage like /scratch or $TMPDIR
    shared-temp-directory: null       # Shared directory currently used to dump temporary security objects for workers
    extra: null                 # deprecated: use worker-extra-args
    worker-command: "distributed.cli.dask_worker" # Command to launch a worker
    worker-extra-args: []       # Additional arguments to pass to `dask-worker`

    # LSF resource manager options
    shebang: "#!/usr/bin/env bash"
    queue: null
    project: null
    walltime: '00:30'
    env-extra: null
    job-script-prologue: []
    ncpus: null
    mem: null
    job-extra: null
    job-extra-directives: []
    job-directives-skip: []
    log-directory: null
    lsf-units: null
    use-stdin: True             # (bool) How jobs are launched, i.e. 'bsub jobscript.sh' or 'bsub < jobscript.sh'

    # Scheduler options
    scheduler-options: {}

  htcondor:
    name: dask-worker

    # Dask worker options
    cores: null                 # Total number of cores per job
    memory: null                # Total amount of memory per job
    processes: null                # Number of Python processes per job

    python: null                # Python executable
    interface: null             # Network interface to use like eth0 or ib0
    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
    local-directory: null       # Location of fast local storage like /scratch or $TMPDIR
    shared-temp-directory: null       # Shared directory currently used to dump temporary security objects for workers
    extra: null                 # deprecated: use worker-extra-args
    worker-command: "distributed.cli.dask_worker" # Command to launch a worker
    worker-extra-args: []       # Additional arguments to pass to `dask-worker`

    # HTCondor Resource Manager options
    disk: null                  # Total amount of disk per job
    env-extra: null
    job-script-prologue: []
    job-extra: null             # Extra submit attributes
    job-extra-directives: {}    # Extra submit attributes
    job-directives-skip: []
    submit-command-extra: []    # Extra condor_submit arguments
    cancel-command-extra: []    # Extra condor_rm arguments
    log-directory: null
    shebang: "#!/usr/bin/env condor_submit"

    # Scheduler options
    scheduler-options: {}

  local:
    name: dask-worker
    # Dask worker options
    cores: null                 # Total number of cores per job
    memory: null                # Total amount of memory per job
    processes: null                # Number of Python processes per job

    python: null                # Python executable
    interface: null             # Network interface to use like eth0 or ib0
    death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
    local-directory: null       # Location of fast local storage like /scratch or $TMPDIR
    shared-temp-directory: null       # Shared directory currently used to dump temporary security objects for workers
    extra: null                 # deprecated: use worker-extra-args
    worker-command: "distributed.cli.dask_worker" # Command to launch a worker
    worker-extra-args: []       # Additional arguments to pass to `dask-worker`

    env-extra: null
    job-script-prologue: []
    job-extra: null
    job-extra-directives: []
    job-directives-skip: []
    log-directory: null

    # Scheduler options
    scheduler-options: {}
